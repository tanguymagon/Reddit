{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Upvote Prediction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=blue> __By Lucas Fabre & Tanguy Magon__ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Import packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tanguy\n",
      "[nltk_data]     Magon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import scipy\n",
    "import emoji\n",
    "import regex\n",
    "import string\n",
    "import textstat\n",
    "import readability\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import time\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Bidirectional, LSTM, Input, Embedding\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Tanguy Magon\\\\Documents\\\\M2\\\\S2\\\\Web_Mining\\\\comments_students.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Cleansing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created_utc seems not irrelevant.\\\n",
    "subreddit_id has always the same value.\\\n",
    "subreddit has always the same value.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(['name','subreddit','subreddit_id','created_utc'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Split into training set and test set__\n",
    "The comments where ups=np.NaN are the comments in the test set. The other comments are in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data['ups'].notnull()]\n",
    "test = data[data['ups'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of nas for each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop nas in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train = train.dropna(how='any', subset=['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anconda3\\lib\\site-packages\\pandas\\core\\generic.py:6130: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.fillna({'body': 'deleted'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ups          1016458\n",
       "link_id            0\n",
       "id                 0\n",
       "author             0\n",
       "body               0\n",
       "parent_id          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train['ups'], kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Feature Creation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=blue> __Structural Features__ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  =  train[~(train.body == ' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def link_presence(data):\n",
    "    \"\"\"Create a variable that accounts how many subreddits are in the link id\"\"\"\n",
    "    data['link_presence'] = data.groupby('link_id')['link_id'].transform('count')\n",
    "    return data\n",
    "\n",
    "def sentiment(data):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    positive = []\n",
    "    neutral = []\n",
    "    negative = []\n",
    "    compound = []\n",
    "    for text in data['body']:\n",
    "        sentiment = sentiment_analyzer.polarity_scores(text)\n",
    "        positive.append(sentiment['pos'])\n",
    "        neutral.append(sentiment['neu'])\n",
    "        negative.append(sentiment['neg'])\n",
    "        compound.append(sentiment['compound'])\n",
    "    data['positive_sentiment'] = pd.Series(positive)\n",
    "    data['neutral_sentiment'] = pd.Series(neutral)\n",
    "    data['negative_sentiment'] = pd.Series(negative)\n",
    "    data['compound_sentiment'] = pd.Series(compound)\n",
    "    return data\n",
    "\n",
    "def author_presence(data):\n",
    "    \"\"\"Create a variable to account for how many times the author posted\"\"\"\n",
    "    data['author_presence'] = data.groupby('author')['author'].transform('count')\n",
    "    return data\n",
    "\n",
    "def parent_presence(data):\n",
    "    \"\"\"Create a variable that accounts how many subreddits are in the parent element ( link or comment)\"\"\"\n",
    "    data['parent_presence'] = data.groupby('author')['author'].transform('count')\n",
    "    return data\n",
    "\n",
    "def reply_to_comment(data):\n",
    "    \"\"\"Create a dummy to know if the author replies to a comment inside a link or a link (1 comment, 0 otherwise)\"\"\"\n",
    "    data['reply_comment'] = np.where(data.link_id != data.parent_id, 1, 0)\n",
    "    return data\n",
    "\n",
    "def percentage_of_uppercases(data):\n",
    "    \"\"\"compute the pourcentage of uppercase letter in a comment\"\"\"\n",
    "    data['percentage_uppercase'] = data['body'].apply(lambda x : 1 if len(list(filter(str.isalpha, x))) == 0 else\n",
    "                   sum(map(str.isupper, list(filter(str.isalpha, x)))) / len(list(filter(str.isalpha, x))))\n",
    "    return data\n",
    "\n",
    "def length_comment(data):\n",
    "    \"\"\"give the number of words of a comment\"\"\"\n",
    "    data['len_comment'] = data['body'].apply(len)\n",
    "    return data\n",
    "\n",
    "def avg_word(data):\n",
    "    \"\"\"average word length\"\"\"\n",
    "    data['avg_word'] = data['body'].apply(lambda x: 1 if len(x.split()) == 0 else sum(len(x.split()) for word in x.split())/len(x.split()))\n",
    "    return data\n",
    "    \n",
    "def readability(data):\n",
    "    \"\"\"readability of a comment\"\"\"\n",
    "    data['readability'] = data['body'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "    return data\n",
    "\n",
    "def question(data):\n",
    "    \"\"\"number of question marks\"\"\"\n",
    "    data['number_of_question'] = data['body'].apply(lambda x : x.count(\"?\"))\n",
    "    return data\n",
    "    \n",
    "def exclamation(data):\n",
    "    \"\"\"number of exclamtion marks\"\"\"\n",
    "    data['number_of_question'] = data['body'].apply(lambda x : x.count(\"!\"))\n",
    "    return data\n",
    "\n",
    "def punctuation(data):\n",
    "    \"\"\"count punctuation\"\"\"\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2]) \n",
    "    data['poncuation'] = data['body'].apply(lambda x: count(x,set(string.punctuation)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def structural_features(data):\n",
    "    \"\"\"Function thats apply the following:\n",
    "        1. link presence\n",
    "        2. sentiment\n",
    "        3. author presence\n",
    "        4. parent presence\n",
    "        5. reply to comment\n",
    "        6. percentage of uppercase\n",
    "        7. avg word\n",
    "        8. readability\n",
    "        9. question\n",
    "        10. exclamation\n",
    "        11. punctuation\"\"\"\n",
    "    structural_df = (data\n",
    "            .pipe(link_presence)\n",
    "            .pipe(sentiment)\n",
    "            .pipe(author_presence)\n",
    "            .pipe(parent_presence)\n",
    "            .pipe(reply_to_comment)\n",
    "            .pipe(percentage_of_uppercases)\n",
    "            .pipe(avg_word)\n",
    "            .pipe(readability)\n",
    "            .pipe(question)\n",
    "            .pipe(exclamation)\n",
    "            .pipe(punctuation))\n",
    "    return structural_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Apply structural_features transformation on train and test.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parallele computing to save time\n",
    "from multiprocessing import Pool\n",
    "p = Pool(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_features = p.map(structural_features, train)\n",
    "test_features = p.map(structural_features, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv('train_up.csv')\n",
    "test_features.to_csv('test_up.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Stylometric characteristics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=blue> __Text processing__ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(data, colname):\n",
    "    \"\"\"remove ponctuation from text\"\"\"\n",
    "    data[colname]= data[colname].str.replace(r\"[^\\w\\s]\",'')\n",
    "    return data\n",
    "\n",
    "def convert_text_to_lowercase(data, colname):\n",
    "    \"\"\"Convert text data to lowercase\"\"\"\n",
    "    data[colname] = data[colname].str.lower()\n",
    "    return data\n",
    "\n",
    "def remove_digits(data, colname):\n",
    "    \"\"\"Remove punctuation ad alphanumeric characters\"\"\"\n",
    "    data[colname] = data[colname].str.replace(r'\\d+','')\n",
    "    return data\n",
    "\n",
    "def tokenize_sentence(df, colname):\n",
    "    \"\"\"Tokenize sentence\"\"\"\n",
    "    df[colname] = df[colname].str.split()\n",
    "    return df\n",
    "\n",
    "def remove_stop_words(df, colname):\n",
    "    \"\"\"Remove stop words\"\"\"\n",
    "    stop_words= stopwords.words('english')\n",
    "    df[colname] = df[colname].dropna().apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    return df\n",
    "\n",
    "def reverse_tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].str.join(' ')\n",
    "    return df\n",
    "\n",
    "def text_preprocessing(df, colname):\n",
    "    \"\"\"pipeline of all the fonctions created:\n",
    "       1. remove puncuation\n",
    "       2. convert texte to lowercase\n",
    "       3. remove digits\n",
    "       4. tokenize sentence\n",
    "       5. remove stop words\n",
    "       6. reverse tokenize sentence\"\"\"\n",
    "    df = (df\n",
    "        .pipe(remove_punctuation, colname)\n",
    "        .pipe(convert_text_to_lowercase, colname)\n",
    "        .pipe(remove_digits, colname)\n",
    "        .pipe(tokenize_sentence, colname)\n",
    "        .pipe(remove_stop_words, colname)\n",
    "        .pipe(reverse_tokenize_sentence, colname))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Apply Text processing transformation on train and test.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = text_preprocessing(train,'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\anconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preprocessed = text_preprocessing(test,'body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Modelisation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Part 1. Modelsation with structural features using CatBoostRegressor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix can give us a good idea about the correlation between each variable and verify if some variables are not too correlated with each other: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "heat = sns.heatmap(corrmat,  square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parent presence and author presence are highly correlated with each other so we drop parent presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"parent_presence\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split our train into two samples to test our model perfomance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = train.drop(['ups',\"link_id\",\"id\",\"author\",\"parent_id\"],axis=1)\n",
    "Y = train['ups']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such regression, and after a benchmark of all the Machine Learning algorithm available, we decided to use CatBoost because it has the best performance among other Gradient Boosting algorithm. \\\n",
    "Also, as the learning is very time consuming, we did not have the time to test all the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did a grid search in order to find the best parameters : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostRegressor()\n",
    "\n",
    "grid = {'learning_rate': [0.03, 0.1],\n",
    "        'depth': [4, 6, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n",
    "\n",
    "grid_search_result = cat.grid_search(grid, X = X_train, y = y_train, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstart= time.time()\n",
    "cb_model = CatBoostRegressor(iterations=700,\n",
    "                             learning_rate=0.03,\n",
    "                             depth=10,\n",
    "                             eval_metric='MAE',\n",
    "                            )\n",
    "cb_model.fit(X_train, y_train,\n",
    "             eval_set=(X_test,y_test),\n",
    "             use_best_model=True,\n",
    "             verbose=True,\n",
    "             plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best MAE was 20.21 which is not good enough for us, and the Kaggle LeaderBoard is much more challenging than that ! \\\n",
    "bestTest = 20.21101802 \\\n",
    "bestIteration = 121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Part 2. Modelsation with text processing using LSTM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the resutls with CatBoost was not good enough, we decided to test a neural network model using Long Short Term Memory networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first need to apply a word embedding using FastText to give a weight matrix to our NN. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # maximum number of words take into account by the model vocabulary\n",
    "VOCAB_SIZE = 50000\n",
    "# maximum number of words per reddit comment\n",
    "MAX_LENGTH = 500\n",
    "# dimension of the embedding layer in the network\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_cleaned['body'].str.split(' ')\n",
    "\n",
    "fastText_model = FastText(\n",
    "    data.astype(str),\n",
    "    size=EMBEDDING_DIM,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function was used to build the embedding matrix with fastText: \\\n",
    "(Taken from our deep-learning classes during the first semester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_with_fastText(tokenizer, word_vectors, vocab_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    Build the embedding matrix with the fastText model to initilize the firt LSTM layer.\n",
    "    \"\"\"\n",
    "    # word index\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # define the number of word to keep (correspond to the input dimension of the model)\n",
    "    nb_words = min(vocab_size, len(word_vectors.vocab)) + 1\n",
    "\n",
    "    # initialize the matrix with random numbers\n",
    "    wv_matrix = (np.random.rand(nb_words, embedding_dim) - 0.5) / 5.0\n",
    "\n",
    "    # feed the matrix using fastText representation\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            wv_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return wv_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilaze embedding matrix with fastText vectorization\n",
    "wv_matrix, input_size = initialize_embedding_with_fastText(\n",
    "    tokenizer,\n",
    "    fastText_model.wv,\n",
    "    VOCAB_SIZE,\n",
    "    EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __LSTM Model architecture__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a NN with 2 bidirectional LSTM hidden Layers followed with BatchNormalization and SpatialDropout.\\\n",
    "Dropout randomly shuts down some neurons in each iteration, we choose to shutdown 0.3 of them \\\n",
    "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. (by subtracting the batch mean and dividing by the batch standard deviation). It consequently stabilize the learning process and reduces the training time. \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our machine was not enough powerful to execute such computation, we run this NN with Google Colab on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_mode_with_fastText(number_of_classes, wv_matrix, VOCAB_SIZE, MAX_LENGTH, EMBEDDING_DIM):\n",
    "\n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "\n",
    "    x = Embedding(input_dim=VOCAB_SIZE,\n",
    "                  output_dim=EMBEDDING_DIM,\n",
    "                  input_length=MAX_LENGTH,\n",
    "                  weights=[wv_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(LSTM(300, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(LSTM(300))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    #x = Bidirectional(LSTM(128))(x)\n",
    "    out = Dense(number_of_classes, kernel_initializer='normal',activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_fastText = build_lstm_mode_with_fastText(\n",
    "    1,\n",
    "    wv_matrix,\n",
    "    input_size,\n",
    "    MAX_LENGTH,\n",
    "    EMBEDDING_DIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm_model_fastText.fit(\n",
    "    padded_data_sequences_train,\n",
    "    train_cleaning.ups,\n",
    "    epochs=2,\n",
    "    batch_size=1024,\n",
    "    verbose=1,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this architecture we get the best results with a MAE on the validation test set of 12.03 ! \\\n",
    "val_loss: 12.0330 - val_mean_absolute_error: 12.0330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Submission process__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(prediction, sub_name):\n",
    "  my_submission = pd.DataFrame({'id':test_cleaning.id,'predicted':prediction})\n",
    "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
    "  print('A submission file has been made')\n",
    "\n",
    "predictions = lstm_model_fastText.predict(test_cleaning)\n",
    "make_submission(predictions[:,0],'submissionLSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried to to a Tf idf model structure with the following function but it was too long to realize and even in google colab we did not have enoug RAM to apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_features(X_train, X_test, ngram = (1,2), maxdf = 0.7, mindf = 10):\n",
    "    \"\"\"X_train, X_test -- samples\n",
    "    return TF-IDF vectorized representation of each sample and vocabulary\"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(token_pattern = '(\\S+)', ngram_range = ngram, max_df = maxdf, min_df = mindf)\n",
    "    txt_fitted = tfidf_vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train = txt_fitted.transform(X_train)\n",
    "    X_test = txt_fitted.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_test, ngram = (1,2), maxdf = 0.99, mindf = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
